{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "505kUrH-TIDC"
      },
      "source": [
        "# Customer Sentiment Reviews on HP Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JyxygocTIDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f7d98d-1a1a-4293-d869-598295cca10d"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.stem.porter import *\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLJ_KiBUTIDk"
      },
      "source": [
        "def simplify(doc):\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A).lower().strip() # Remove special characters, whitespaces and make lower case\n",
        "    tokens = nltk.WordPunctTokenizer().tokenize(doc) # Tokenize\n",
        "    filtered_tokens = [token for token in tokens if token not in nltk.corpus.stopwords.words('english')] # Remove stopwords\n",
        "    doc = ' '.join(filtered_tokens) # Re-create document from filtered tokens\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfyFYzPTTIDy"
      },
      "source": [
        "def get_words(sentence):\n",
        "    stemmer = PorterStemmer()#Stemming is the process of producing morphological variants of a root/base word\n",
        "    words = [stemmer.stem(x) for x in simplify(sentence).split()]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFoWe_TYTID7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ktawd5dTID9"
      },
      "source": [
        "train = open('training_set.csv', 'r') # Training data\n",
        "train.readline() # Read and remove header row\n",
        "word_sentiment = {} # Dictionary stores sentiment weight of all words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfcN4FZvTIEF"
      },
      "source": [
        "for data in train:\n",
        "    sentiment, line = data.split(',')\n",
        "    words = get_words(line)\n",
        "    for word in words:\n",
        "        try: # Increment weight of the word by 1 if positive, decrement weight by 1 if negative\n",
        "            word_sentiment[word][0] = word_sentiment[word][0] + 1 if int(sentiment) == 1 else word_sentiment[word][0] - 1\n",
        "        except: # If word doesn't exist, create new entry in dictionary\n",
        "            word_sentiment[word] = [1, 0] if int(sentiment) == 1 else [-1, 0]\n",
        "        finally: # Increment number of occurences of the word; used later to compute the weighted sum instead of just sum\n",
        "            word_sentiment[word][1] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "V6LEVorqTIEL"
      },
      "source": [
        "word_weighted_sentiment = {word: word_sentiment[word][0] / word_sentiment[word][1] for word in word_sentiment.keys()}\n",
        "# Weighted sum of the sentiment of each word (divide weight of the word by number of occurences of the word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGE-Ht4hTIES"
      },
      "source": [
        "train.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heLuytLqTIEX"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rNC-vE-TIEY"
      },
      "source": [
        "test = open('test_set.csv', 'r') # Testing data\n",
        "test.readline() # Read and remove header row\n",
        "output = open('prediction_file.csv', 'a') # Output file (testing data with predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG06s5eyTIEc"
      },
      "source": [
        "for data in test:\n",
        "    sentiment = 0\n",
        "    words = get_words(data)\n",
        "    for word in words:\n",
        "        try:\n",
        "            sentiment += word_weighted_sentiment[word] # Compute sum of weighted sentiments of the words in the review\n",
        "        except:\n",
        "            sentiment += 0 # MISSING WORDS (words in testing data but not training data) ARE GIVEN SENTIMENT OF 0\n",
        "    if sentiment >= 0:\n",
        "        output.write('1,' + data + '\\n')\n",
        "    else:\n",
        "        output.write('0,' + data + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky0vRIzKTIEf"
      },
      "source": [
        "test.close()\n",
        "output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJK8tgBOTIEk"
      },
      "source": [
        "The results of the predictions may be observed in `prediction_file.csv`."
      ]
    }
  ]
}